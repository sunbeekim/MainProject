from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Dict
import uvicorn
from fastapi.middleware.cors import CORSMiddleware
import re
import asyncio

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "https://sunbee.world",
        "https://www.sunbee.world",
        "http://localhost:8081",
        "http://localhost:8082"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

print("ëª¨ë¸ ë¡œë”© ì¤‘...")
# ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì‚¬ìš© (ë¼ì´ì„¼ìŠ¤ ì œí•œ ì—†ìŒ)
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
)
print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

class ChatRequest(BaseModel):
    message: str
    history: list = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€

@app.post("/api/fastapi/chat")
async def chat(request: ChatRequest) -> Dict[str, str]:
    try:
        # ë” êµ¬ì²´ì ì¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_prompt = """You are a knowledgeable AI assistant. 

1. Always respond clearly.
2. Answer consistently, taking into account the context of the previous conversation.
3. Content unrelated to the question will not be answered.
4. Professional content is explained accurately and in detail.
5. Answer "I don't know" to questions you don't know."""

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"<system>{system_prompt}</system>\n"

        print("\n=== ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‹œì‘ ===")
        print(f"íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ ìˆ˜: {len(request.history)}")

        # ì´ì „ ëŒ€í™” ë‚´ìš© ì¶”ê°€ (ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©)
        for i, msg in enumerate(request.history[-4:]):
            print(f"\në©”ì‹œì§€ {i+1}:")
            print(f"ì‚¬ìš©ì: {msg['user']}")
            if "assistant" in msg:
                print(f"ì–´ì‹œìŠ¤í„´íŠ¸: {msg['assistant']}")
            full_prompt += f"<user>{msg['user']}</user>\n"
            if "assistant" in msg:
                full_prompt += f"<assistant>{msg['assistant']}</assistant>\n"

        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        print(f"\ní˜„ì¬ ë©”ì‹œì§€: {request.message}")
        full_prompt += f"<user>{request.message}</user>\n<assistant>"

        print("\n=== ìµœì¢… í”„ë¡¬í”„íŠ¸ ===")
        print(full_prompt)
        print("=====================\n")

        # ì…ë ¥ ë©”ì‹œì§€ í† í°í™”
        inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        generation_config = {
            "max_length": 2048,
            "temperature": 0.7,
            "top_p": 0.95,
            "repetition_penalty": 1.15,
            "do_sample": True
        }

        # ğŸ”¹ ë¹„ë™ê¸° ì‹¤í–‰ìœ¼ë¡œ ë³€ê²½ (`asyncio.to_thread` ì‚¬ìš©)
        outputs = await asyncio.to_thread(
            model.generate,
            **inputs,
            **generation_config,
            pad_token_id=tokenizer.eos_token_id
        )

        # ì‘ë‹µ ë””ì½”ë”© ë° í”„ë¡¬í”„íŠ¸ ì œê±°
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("<assistant>")[-1].strip()
        clean_response = re.sub(r"</?[a-zA-Z0-9]+>", "", response).strip()
        print(clean_response)
        return {"response": clean_response}

    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)